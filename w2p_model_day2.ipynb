{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d29e0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d4bcc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# Load T5-Base Model\n",
    "# ---------------------\n",
    "summarizer_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "\n",
    "summarizer_model.eval()\n",
    "for param in summarizer_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9be291f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded nid2body with 113762 items\n",
      "üßæ Sample NID: N10000\n",
      "üìù Headline: Predicting Atlanta United's lineup against Columbus Crew in the U.S. Open Cup\n"
     ]
    }
   ],
   "source": [
    "# Load nid2body from pickle\n",
    "with open(\"nid2body.pkl\", \"rb\") as f:\n",
    "    nid2body = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded nid2body with {len(nid2body)} items\")\n",
    "sample_nid = list(nid2body.keys())[0]\n",
    "print(f\"üßæ Sample NID: {sample_nid}\\nüìù Headline: {nid2body[sample_nid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08fb8c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded sid2sum with 135001 items\n",
      "üßæ Sample SID: S-1\n",
      "üìù Summary: The officer reportedly also pointed his gun at Harper and her children.\n"
     ]
    }
   ],
   "source": [
    "# Load sid2sum from pickle\n",
    "with open(\"sid2sum.pkl\", \"rb\") as f:\n",
    "    sid2sum = pickle.load(f)\n",
    "\n",
    "# Debug print\n",
    "print(f\"‚úÖ Loaded sid2sum with {len(sid2sum)} items\")\n",
    "sample_sid = list(sid2sum.keys())[0]\n",
    "print(f\"üßæ Sample SID: {sample_sid}\\nüìù Summary: {sid2sum[sample_sid][:300]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e60dcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Device and Precision Setup ===\n",
    "torch.set_default_dtype(torch.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dim = 768\n",
    "\n",
    "# === Utility Functions ===\n",
    "def get_embedding(key, table, dim):\n",
    "    if key not in table:\n",
    "        table[key] = torch.nn.Parameter(torch.randn(dim, dtype=torch.float32, device=device) * 0.01, requires_grad=True)\n",
    "    return table[key]\n",
    "\n",
    "\n",
    "# === Load Embeddings ===\n",
    "with open(\"summary_T5.pkl\", \"rb\") as f:\n",
    "    summary_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"newsbody_T5.pkl\", \"rb\") as f:\n",
    "    newsbody_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "with open(\"headline_T5.pkl\", \"rb\") as f:\n",
    "    headline_embed = {k: torch.tensor(v, dtype=torch.float32, device=device) for k, v in pickle.load(f).items()}\n",
    "\n",
    "embed_tables = {\n",
    "    'summary': summary_embed,\n",
    "    'newsbody': newsbody_embed,\n",
    "    'headline': headline_embed\n",
    "}\n",
    "\n",
    "# === Load Dataset ===\n",
    "lookup_df = pd.read_csv(\"w2p_engage_list.csv\").set_index('EdgeID')\n",
    "train_df = pd.read_csv(\"train_w2p.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "083823d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>UserID</th>\n",
       "      <th>EHist</th>\n",
       "      <th>EPos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>U10000_1</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>U10000_2</td>\n",
       "      <td>['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...</td>\n",
       "      <td>E133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>U100006_1</td>\n",
       "      <td>['E151']</td>\n",
       "      <td>E152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>U100006_2</td>\n",
       "      <td>['E151', 'E152', 'E153', 'E154', 'E155', 'E156...</td>\n",
       "      <td>E168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>U100006_3</td>\n",
       "      <td>['E151', 'E152', 'E153', 'E154', 'E155', 'E156...</td>\n",
       "      <td>E230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>195583</td>\n",
       "      <td>U233775_3</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>195584</td>\n",
       "      <td>U233775_4</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>195585</td>\n",
       "      <td>U233775_5</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>195586</td>\n",
       "      <td>U233775_6</td>\n",
       "      <td>['E1882808', 'E1882809', 'E1882810', 'E1882811...</td>\n",
       "      <td>E1882920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>195587</td>\n",
       "      <td>U233777_1</td>\n",
       "      <td>['E1882958', 'E1882959', 'E1882960', 'E1882961...</td>\n",
       "      <td>E1883030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0     UserID  \\\n",
       "0               2   U10000_1   \n",
       "1               3   U10000_2   \n",
       "2              11  U100006_1   \n",
       "3              12  U100006_2   \n",
       "4              13  U100006_3   \n",
       "...           ...        ...   \n",
       "49995      195583  U233775_3   \n",
       "49996      195584  U233775_4   \n",
       "49997      195585  U233775_5   \n",
       "49998      195586  U233775_6   \n",
       "49999      195587  U233777_1   \n",
       "\n",
       "                                                   EHist      EPos  \n",
       "0      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...       E84  \n",
       "1      ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8...      E133  \n",
       "2                                               ['E151']      E152  \n",
       "3      ['E151', 'E152', 'E153', 'E154', 'E155', 'E156...      E168  \n",
       "4      ['E151', 'E152', 'E153', 'E154', 'E155', 'E156...      E230  \n",
       "...                                                  ...       ...  \n",
       "49995  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882859  \n",
       "49996  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882866  \n",
       "49997  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882869  \n",
       "49998  ['E1882808', 'E1882809', 'E1882810', 'E1882811...  E1882920  \n",
       "49999  ['E1882958', 'E1882959', 'E1882960', 'E1882961...  E1883030  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=train_df[:50000]\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bbe86336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Head</th>\n",
       "      <th>Relation</th>\n",
       "      <th>Tail</th>\n",
       "      <th>User</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EdgeID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E1</th>\n",
       "      <td>0</td>\n",
       "      <td>U10000</td>\n",
       "      <td>skip</td>\n",
       "      <td>N110699</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E2</th>\n",
       "      <td>1</td>\n",
       "      <td>N110699</td>\n",
       "      <td>skip</td>\n",
       "      <td>N104733</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E3</th>\n",
       "      <td>2</td>\n",
       "      <td>N104733</td>\n",
       "      <td>skip</td>\n",
       "      <td>N80645</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E4</th>\n",
       "      <td>3</td>\n",
       "      <td>N80645</td>\n",
       "      <td>skip</td>\n",
       "      <td>N76869</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E5</th>\n",
       "      <td>4</td>\n",
       "      <td>N76869</td>\n",
       "      <td>skip</td>\n",
       "      <td>N119531</td>\n",
       "      <td>U10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413173</th>\n",
       "      <td>18413172</td>\n",
       "      <td>N107780</td>\n",
       "      <td>click</td>\n",
       "      <td>N97051</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413174</th>\n",
       "      <td>18413173</td>\n",
       "      <td>N97051</td>\n",
       "      <td>click</td>\n",
       "      <td>N81002</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413175</th>\n",
       "      <td>18413174</td>\n",
       "      <td>N81002</td>\n",
       "      <td>click</td>\n",
       "      <td>N11725</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413176</th>\n",
       "      <td>18413175</td>\n",
       "      <td>N11725</td>\n",
       "      <td>click</td>\n",
       "      <td>N89229</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E18413177</th>\n",
       "      <td>18413176</td>\n",
       "      <td>N89229</td>\n",
       "      <td>skip</td>\n",
       "      <td>N67207</td>\n",
       "      <td>U249644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18413177 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Unnamed: 0     Head Relation     Tail     User\n",
       "EdgeID                                                   \n",
       "E1                  0   U10000     skip  N110699   U10000\n",
       "E2                  1  N110699     skip  N104733   U10000\n",
       "E3                  2  N104733     skip   N80645   U10000\n",
       "E4                  3   N80645     skip   N76869   U10000\n",
       "E5                  4   N76869     skip  N119531   U10000\n",
       "...               ...      ...      ...      ...      ...\n",
       "E18413173    18413172  N107780    click   N97051  U249644\n",
       "E18413174    18413173   N97051    click   N81002  U249644\n",
       "E18413175    18413174   N81002    click   N11725  U249644\n",
       "E18413176    18413175   N11725    click   N89229  U249644\n",
       "E18413177    18413176   N89229     skip   N67207  U249644\n",
       "\n",
       "[18413177 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43956d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------\n",
    "# # Build Tail2Idx\n",
    "# # ---------------------\n",
    "# tail_set = set()\n",
    "\n",
    "# print(\"Building Tail2Idx from EHist and EPos...\")\n",
    "# for row in tqdm(train_df.itertuples(), total=len(train_df), desc=\"Collecting tails\"):\n",
    "#     try:\n",
    "#         bhist = literal_eval(row.EHist)\n",
    "#         bpos = row.EPos\n",
    "\n",
    "#         # Add tails from Bhist\n",
    "#         for b_id in bhist:\n",
    "#             if b_id in lookup_df.index:\n",
    "#                 tail = lookup_df.loc[b_id, 'Tail']\n",
    "#                 tail_set.add(tail)\n",
    "\n",
    "#         # Add tail from Bpos\n",
    "#         if bpos in lookup_df.index:\n",
    "#             tail = lookup_df.loc[bpos, 'Tail']\n",
    "#             tail_set.add(tail)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"[Skip] Error in row {row.Index}: {e}\")\n",
    "\n",
    "# # === Final mappings ===\n",
    "# tail_ids = sorted(tail_set)\n",
    "# tail2idx = {tid: idx for idx, tid in enumerate(tail_ids)}\n",
    "# idx2tail = {idx: tid for tid, idx in tail2idx.items()}\n",
    "\n",
    "# print(f\"‚úÖ Tail2Idx built with {len(tail2idx)} unique tail IDs.\")\n",
    "\n",
    "# # ---------------------\n",
    "# # Save as pickle\n",
    "# # ---------------------\n",
    "# with open(\"tail_mappings.pkl\", \"wb\") as f:\n",
    "#     pickle.dump({\"tail2idx\": tail2idx, \"idx2tail\": idx2tail}, f)\n",
    "\n",
    "# print(\"üíæ Saved tail2idx and idx2tail to tail_mappings.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca969e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded tail2idx with 544137 entries\n",
      "üßæ Sample tail ‚Üí idx: [('N10000', 0), ('N100001', 1), ('N100003', 2)]\n",
      "üßæ Sample idx ‚Üí tail: [(0, 'N10000'), (1, 'N100001'), (2, 'N100003')]\n"
     ]
    }
   ],
   "source": [
    "# ---------------------\n",
    "# Load back from pickle\n",
    "# ---------------------\n",
    "with open(\"tail_mappings.pkl\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "tail2idx = mappings[\"tail2idx\"]\n",
    "idx2tail = mappings[\"idx2tail\"]\n",
    "\n",
    "print(f\"‚úÖ Loaded tail2idx with {len(tail2idx)} entries\")\n",
    "print(f\"üßæ Sample tail ‚Üí idx: {list(tail2idx.items())[:3]}\")\n",
    "print(f\"üßæ Sample idx ‚Üí tail: {list(idx2tail.items())[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5378608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Sequence Engagement/Behavior Encoder\n",
    "# -------------------------\n",
    "\n",
    "class BehaviorEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, tail2idx, device, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "        self.tail2idx = tail2idx\n",
    "\n",
    "        # === Base action vectors ===\n",
    "        self.e_clk = nn.Parameter(torch.tensor([1., 0., 0., 0.], device=device))\n",
    "        self.e_skp = nn.Parameter(torch.tensor([0., 1., 0., 0.], device=device))\n",
    "        self.e_gensumm = nn.Parameter(torch.tensor([0., 0., 1., 0.], device=device))\n",
    "        self.e_sumgen = nn.Parameter(torch.tensor([0., 0., 0., 1.], device=device))\n",
    "\n",
    "        # Action-specific transforms\n",
    "        self.W_clk = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_skp = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_gensumm = nn.Linear(4, hidden_dim, bias=False)\n",
    "        self.W_sumgen = nn.Linear(4, hidden_dim, bias=False)\n",
    "\n",
    "        # State transforms\n",
    "        self.W_pull = nn.Linear(1, hidden_dim, bias=False)\n",
    "        self.W_s = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_d = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "        # Fusion\n",
    "        self.Wh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.Wz = nn.Linear(hidden_dim, 3, bias=False)\n",
    "        self.b_z = nn.Parameter(torch.zeros(3, device=device))\n",
    "        self.W_emb = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.b_emb = nn.Parameter(torch.zeros(hidden_dim, device=device))\n",
    "\n",
    "        # Rotation/translation\n",
    "        self.W_angle = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_theta = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        self.W_h = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        self.W_m = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Scalars\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "        self.beta = nn.Parameter(torch.tensor(0.5, device=device))\n",
    "\n",
    "        # Classifier head over tails\n",
    "        self.classifier = nn.Linear(hidden_dim, len(tail2idx))\n",
    "\n",
    "        # Next-step prediction head\n",
    "        self.W_next = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def _show(self, name, tensor, maxlen=6):\n",
    "        if not self.debug: \n",
    "            return\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            flat = tensor.detach().cpu().numpy().flatten()\n",
    "            vals = \", \".join(f\"{x:.4f}\" for x in flat[:maxlen])\n",
    "            if len(flat) > maxlen: vals += \", ...\"\n",
    "            print(f\"{name} (shape={tuple(tensor.shape)}): [{vals}]\")\n",
    "        else:\n",
    "            print(f\"{name}: {tensor}\")\n",
    "\n",
    "    def softmin_pool(self, a, b):\n",
    "        return -self.alpha * torch.log(torch.exp(a / self.alpha) +\n",
    "                                       torch.exp(b / self.alpha) + 1e-9)\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables):\n",
    "        total_loss = torch.tensor(0., dtype=torch.float32, device=self.device)\n",
    "\n",
    "        # === PASS 1: raw step embeddings (E_seq) ===\n",
    "        E_seq = []\n",
    "        h_clk = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h_skp = torch.zeros(self.hidden_dim, device=self.device)\n",
    "        h = torch.zeros(self.hidden_dim, device=self.device)\n",
    "\n",
    "        for t, b_id in enumerate(Bhist):\n",
    "            if b_id not in lookup_df.index:\n",
    "                continue\n",
    "            row = lookup_df.loc[b_id]\n",
    "            tail_id, rel = row['Tail'], row['Relation']\n",
    "\n",
    "            d_i = embed_tables['newsbody'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            s_i = embed_tables['summary'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "            d_i_title = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "\n",
    "            # init state\n",
    "            if t == 0:\n",
    "                head_emb = embed_tables['headline'].get(tail_id, torch.zeros(self.hidden_dim, device=self.device))\n",
    "                h_clk, h_skp = head_emb, head_emb\n",
    "                h = torch.sigmoid(self.W_s(head_emb)) * h_clk + (1 - torch.sigmoid(self.W_s(head_emb))) * h_skp\n",
    "\n",
    "            # relation-specific context c_i\n",
    "            if rel == \"click\":\n",
    "                c_i = (self.W_clk.weight @ self.e_clk * h) * d_i\n",
    "            elif rel == \"skip\":\n",
    "                d_ip1 = torch.zeros_like(d_i)\n",
    "                if t+1 < len(Bhist) and Bhist[t+1] in lookup_df.index:\n",
    "                    d_ip1 = embed_tables['newsbody'].get(\n",
    "                        lookup_df.loc[Bhist[t+1]]['Head'],\n",
    "                        torch.zeros_like(d_i)\n",
    "                    )\n",
    "                pull_term = self.W_pull(torch.tensor([[torch.dot(h_clk, d_ip1)+torch.dot(h_skp, d_i)]],\n",
    "                                                     device=self.device)).squeeze(0)\n",
    "                c_i = torch.tanh(self.W_skp.weight @ self.e_skp + d_i + pull_term) * h * d_i\n",
    "            elif rel == \"gen_summ\":\n",
    "                c_i = (self.W_gensumm.weight @ self.e_gensumm * h) * d_i_title\n",
    "            elif rel == \"summ_gen\":\n",
    "                gate_summgen = self.W_s(self.W_sumgen.weight @ self.e_sumgen)\n",
    "                c_i = self.softmin_pool(gate_summgen * s_i, (1 - gate_summgen) * d_i)\n",
    "                h_clk = h_clk + self.W_d((torch.ones_like(d_i_title) - d_i_title) * s_i)\n",
    "            else:\n",
    "                c_i = d_i\n",
    "\n",
    "            # update hidden\n",
    "            z_i = self.Wh(h) + self.Wc(c_i)\n",
    "            p_i = torch.softmax(self.Wz(z_i) + self.b_z, dim=-1)\n",
    "            m_i = p_i[0]*0.1 + p_i[1]*0.5 + p_i[2]*0.9\n",
    "            if rel == \"click\": h_clk = h_clk + m_i * c_i\n",
    "            elif rel == \"skip\": h_skp = h_skp * (1 - m_i) + c_i\n",
    "            h = self.beta * h_clk + (1 - self.beta) * h_skp\n",
    "\n",
    "            e_i = torch.tanh(self.W_emb(c_i) + self.b_emb)\n",
    "            E_seq.append(e_i)\n",
    "\n",
    "        if not E_seq:\n",
    "            return torch.zeros(self.hidden_dim, device=self.device), None, total_loss\n",
    "\n",
    "        # === PASS 2: contextualize (E‚Ä≤_seq) ===\n",
    "        Eprime_seq = []\n",
    "        eps = 1e-9\n",
    "        for i, e_i in enumerate(E_seq):\n",
    "            if i == 0:\n",
    "                e_prime = e_i\n",
    "            else:\n",
    "                e_prev, e_prime_prev = E_seq[i-1], Eprime_seq[-1]\n",
    "                theta_i = math.pi * torch.tanh(self.W_theta(torch.sigmoid(self.W_angle(e_prime_prev))))\n",
    "                m_i = F.softplus(self.W_m(self.W_h(e_prime_prev)))\n",
    "\n",
    "                v_i = (e_i - e_prime_prev) / (e_i - e_prime_prev).norm(p=2).clamp(min=eps)\n",
    "                u_prev = e_prev / e_prime_prev.norm(p=2).clamp(min=eps)\n",
    "                o_i = (v_i - torch.dot(v_i, u_prev) * u_prev)\n",
    "                o_i = o_i / o_i.norm(p=2).clamp(min=eps)\n",
    "\n",
    "                e_prime = e_prime_prev + m_i * (torch.cos(theta_i) * u_prev + torch.sin(theta_i) * o_i).squeeze()\n",
    "\n",
    "            Eprime_seq.append(e_prime)\n",
    "\n",
    "            # per-step auxiliary loss\n",
    "            tail_id = lookup_df.loc[Bhist[i]]['Tail']\n",
    "            if tail_id in self.tail2idx:\n",
    "                logits_step = self.classifier(e_prime.unsqueeze(0))\n",
    "                target_step = torch.tensor([self.tail2idx[tail_id]], device=self.device)\n",
    "                total_loss = total_loss + F.cross_entropy(logits_step, target_step)\n",
    "\n",
    "        # === Final prediction on Bpos ===\n",
    "        eprime_last = Eprime_seq[-1]\n",
    "\n",
    "        # Next-step embedding prediction\n",
    "        eprime_next = self.W_next(eprime_last)\n",
    "\n",
    "        logits_pos = None\n",
    "        if Bpos in lookup_df.index:\n",
    "            tail_id_pos = lookup_df.loc[Bpos]['Tail']\n",
    "            if tail_id_pos in self.tail2idx:\n",
    "                logits_pos = self.classifier(eprime_next.unsqueeze(0))\n",
    "                target_pos = torch.tensor([self.tail2idx[tail_id_pos]], device=self.device)\n",
    "                total_loss = total_loss + F.cross_entropy(logits_pos, target_pos)\n",
    "\n",
    "        return eprime_last, eprime_next, logits_pos, total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5aa1ceed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Inverse decoder for a single predicted embedding (d_next_pred as e'_pos)\n",
    "# -------------------------\n",
    "class BehaviorInverseDecoderPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Inverse mapping that takes a single predicted e' (embedding for Bpos)\n",
    "    and the head/headline embedding for Bpos, and returns:\n",
    "      - c'_pos (approx pseudo-content)\n",
    "      - s_hat_pos (approx summary)\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim, device, debug=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.debug = debug\n",
    "\n",
    "        # Learnable pseudo-inverse / disentanglers\n",
    "        self.W_emb_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)   # approx W_emb^+\n",
    "        self.W1_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # remove head contribution\n",
    "        self.W2_pinv = nn.Linear(hidden_dim, hidden_dim, bias=False)      # map residual -> summary\n",
    "\n",
    "    def _show(self, name, tensor, maxlen=6):\n",
    "        if not self.debug:\n",
    "            return\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            if tensor.ndim == 0:\n",
    "                print(f\"{name}: {tensor.item():.6f}\")\n",
    "            else:\n",
    "                flat = tensor.detach().cpu().numpy().flatten()\n",
    "                vals = \", \".join(f\"{x:.6f}\" for x in flat[:maxlen])\n",
    "                if len(flat) > maxlen: vals += \", ...\"\n",
    "                print(f\"{name} (shape={tuple(tensor.shape)}): [{vals}]\")\n",
    "        else:\n",
    "            print(f\"{name}: {tensor}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def atanh_safe(x, eps=1e-6):\n",
    "        x = x.clamp(-1+eps, 1-eps)\n",
    "        return 0.5 * torch.log((1+x) / (1-x))\n",
    "\n",
    "    def forward(self, eprime_pos, b_emb, h_pos):\n",
    "        \"\"\"\n",
    "        eprime_pos: tensor (hidden_dim,) -- predicted e' embedding for Bpos\n",
    "        b_emb: encoder bias b_emb (tensor (hidden_dim,))\n",
    "        h_pos: head/headline embedding for Bpos (tensor (hidden_dim,))\n",
    "        Returns: c_prime_pos, s_hat_pos\n",
    "        \"\"\"\n",
    "        # 1) invert embedding nonlinearity: atanh(e') - b_emb\n",
    "        x = self.atanh_safe(eprime_pos) - b_emb  # (hidden_dim,)\n",
    "        c_prime_pos = self.W_emb_pinv(x)         # approx c'_pos\n",
    "\n",
    "        # 2) subtract head contribution and map to summary\n",
    "        residual = c_prime_pos - self.W1_pinv(h_pos)\n",
    "        s_hat_pos = self.W2_pinv(residual)\n",
    "\n",
    "        # debug prints\n",
    "        self._show(\"eprime_pos\", eprime_pos)\n",
    "        self._show(\"atanh(eprime_pos)-b_emb\", x)\n",
    "        self._show(\"c'_pos\", c_prime_pos)\n",
    "        self._show(\"residual (c' - W1^+ h)\", residual)\n",
    "        self._show(\"s_hat_pos\", s_hat_pos)\n",
    "\n",
    "        return c_prime_pos, s_hat_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e250e288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonalizedT5Summarizer(nn.Module):\n",
    "    def __init__(self, hidden_dim, t5_model, behavior_encoder, inverse_decoder, device):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.t5 = t5_model.eval()\n",
    "        self.behavior_encoder = behavior_encoder\n",
    "        self.inverse_decoder = inverse_decoder\n",
    "        self.device = device\n",
    "\n",
    "        # Freeze T5\n",
    "        for param in self.t5.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Learnable gates and attention transforms\n",
    "        self.W_prime = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_qry = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_key = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "        self.W_val = nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50):\n",
    "        # ----------------------\n",
    "        # 1Ô∏è‚É£ Behavior Encoder\n",
    "        # ----------------------\n",
    "        eprime_last, eprime_next, logits_pos, enc_loss = self.behavior_encoder(\n",
    "            Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    "        )\n",
    "        print(\"‚úÖ Behavior Encoder:\")\n",
    "        print(f\"  eprime_last shape: {tuple(eprime_last.shape)}\")\n",
    "        print(f\"  Encoder loss: {enc_loss.item():.4f}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 2Ô∏è‚É£ Inverse Decoder ‚Üí s_hat\n",
    "        # ----------------------\n",
    "        head_emb = embed_tables['headline'].get(Bpos, torch.zeros(self.hidden_dim, device=self.device))\n",
    "        _, s_hat = self.inverse_decoder(eprime_last, self.behavior_encoder.b_emb, head_emb)\n",
    "        print(\"‚úÖ Inverse Decoder:\")\n",
    "        print(f\"  s_hat shape: {tuple(s_hat.shape)}\")\n",
    "        print(f\"  s_hat sample (first 6 dims): {s_hat.detach().cpu().numpy()[:6]}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 3Ô∏è‚É£ Personalized T5 Encoder\n",
    "        # ----------------------\n",
    "        if Bpos not in lookup_df.index:\n",
    "            print(\"‚ö†Ô∏è Bpos not in lookup_df; returning encoder loss only.\")\n",
    "            return enc_loss, None\n",
    "\n",
    "        tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "        doc_text = nid2body.get(tail_id, \" \")\n",
    "\n",
    "        input_text = \"generate headline for: \" + doc_text\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            encoder_outputs = self.t5.encoder(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        enc_states = encoder_outputs.last_hidden_state  # (1, seq_len, hidden_dim)\n",
    "        print(\"‚úÖ Raw T5 Encoder States:\")\n",
    "        print(f\"  Shape: {enc_states.shape}\")\n",
    "        print(f\"  First token (first 10 dims): {enc_states[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # Apply gated eprime_last\n",
    "        gated_eprime = torch.sigmoid(self.W_prime(eprime_last))  # (hidden_dim,)\n",
    "        enc_states_gated = enc_states * gated_eprime.unsqueeze(0).unsqueeze(0)\n",
    "        print(\"‚úÖ Gated/Contextualized Encoder States:\")\n",
    "        print(f\"  First token (first 10 dims): {enc_states_gated[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # Inject s_hat\n",
    "        key = self.W_key(s_hat).unsqueeze(0).unsqueeze(1)   # (1,1,hidden_dim)\n",
    "        value = self.W_val(s_hat).unsqueeze(0).unsqueeze(1) # (1,1,hidden_dim)\n",
    "        seq_len = enc_states_gated.size(1)\n",
    "        key_exp = key.expand(-1, seq_len, -1)\n",
    "        value_exp = value.expand(-1, seq_len, -1)\n",
    "        personalized_enc_states = enc_states_gated + key_exp + value_exp\n",
    "        print(\"‚úÖ Personalized Encoder States (after s_hat injection):\")\n",
    "        print(f\"  First token (first 10 dims): {personalized_enc_states[0,0,:10].detach().cpu().numpy()}\")\n",
    "\n",
    "        # ----------------------\n",
    "        # 4Ô∏è‚É£ Decoder with gold summary\n",
    "        # ----------------------\n",
    "        gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "        print(\"Gold summary source:\",tail_id)\n",
    "        print(\"Gold reference summaries:\",gold_summary_text)\n",
    "        # if gold_summary_text == \"\":\n",
    "        #     print(\"‚ö†Ô∏è Gold summary missing; returning encoder loss only.\")\n",
    "        #     return enc_loss, None\n",
    "\n",
    "        decoder_inputs = tokenizer(\n",
    "            gold_summary_text, return_tensors=\"pt\", max_length=max_len, truncation=True\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(\"‚úÖ Target Summary Tokens:\")\n",
    "        print(f\"  Token IDs: {decoder_inputs.input_ids[0].detach().cpu().numpy()[:min(10, decoder_inputs.input_ids.size(1))]}\")\n",
    "\n",
    "        outputs = self.t5(\n",
    "            input_ids=decoder_inputs.input_ids,\n",
    "            attention_mask=decoder_inputs.attention_mask,\n",
    "            encoder_outputs=(personalized_enc_states,),\n",
    "            labels=decoder_inputs.input_ids\n",
    "        )\n",
    "\n",
    "        pred_tokens = torch.argmax(outputs.logits, dim=-1)\n",
    "        print(\"‚úÖ Predicted Summary Tokens:\")\n",
    "        print(f\"  Token IDs: {pred_tokens[0,:min(10, pred_tokens.size(1))].detach().cpu().numpy()}\")\n",
    "\n",
    "        total_loss = enc_loss + outputs.loss\n",
    "        print(\"‚úÖ Losses:\")\n",
    "        print(f\"  Generation loss (T5): {outputs.loss.item():.4f}\")\n",
    "        print(f\"  Behavior encoder loss: {enc_loss.item():.4f}\")\n",
    "        print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "\n",
    "        return total_loss, outputs.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "74e7fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Random Row UserID: U180417_2\n",
      "üìù Sample Bhist: ['E1133134', 'E1133135', 'E1133136', 'E1133137', 'E1133138', 'E1133139', 'E1133140', 'E1133141', 'E1133142', 'E1133143', 'E1133144']\n",
      "üéØ Target Bpos: E1133145\n",
      "\n",
      "‚úÖ Behavior Encoder Output:\n",
      "  eprime_last shape: torch.Size([768])\n",
      "  eprime_next shape: torch.Size([768])\n",
      "  Predicted tail from encoder: S-204452\n",
      "eprime_pos (shape=(768,)): [0.087004, -0.036369, -0.119065, 0.031284, 0.091543, 0.012611, ...]\n",
      "atanh(eprime_pos)-b_emb (shape=(768,)): [0.087225, -0.036385, -0.119632, 0.031294, 0.091800, 0.012611, ...]\n",
      "c'_pos (shape=(768,)): [-0.047746, -0.034158, 0.023216, -0.038377, 0.071889, 0.035393, ...]\n",
      "residual (c' - W1^+ h) (shape=(768,)): [-0.047746, -0.034158, 0.023216, -0.038377, 0.071889, 0.035393, ...]\n",
      "s_hat_pos (shape=(768,)): [-0.023668, -0.039272, 0.013796, -0.015831, 0.002949, 0.016757, ...]\n",
      "\n",
      "‚úÖ Inverse Decoder Output:\n",
      "  c'_pos (first 6 dims): [-0.04774594 -0.03415801  0.02321595 -0.03837652  0.0718895   0.03539296]\n",
      "  s_hat_pos (first 6 dims): [-0.02366759 -0.0392725   0.01379645 -0.0158311   0.00294878  0.0167574 ]\n",
      "Target summary id S-29150\n",
      "‚úÖ Behavior Encoder:\n",
      "  eprime_last shape: (768,)\n",
      "  Encoder loss: 158.4887\n",
      "eprime_pos (shape=(768,)): [0.242485, -0.055582, -0.108580, -0.078905, 0.021010, 0.162758, ...]\n",
      "atanh(eprime_pos)-b_emb (shape=(768,)): [0.247412, -0.055639, -0.109010, -0.079069, 0.021013, 0.164218, ...]\n",
      "c'_pos (shape=(768,)): [0.091731, 0.026960, -0.034590, -0.025357, -0.062675, -0.023845, ...]\n",
      "residual (c' - W1^+ h) (shape=(768,)): [0.091731, 0.026960, -0.034590, -0.025357, -0.062675, -0.023845, ...]\n",
      "s_hat_pos (shape=(768,)): [0.011239, -0.023251, 0.007267, -0.024996, 0.042779, 0.056156, ...]\n",
      "‚úÖ Inverse Decoder:\n",
      "  s_hat shape: (768,)\n",
      "  s_hat sample (first 6 dims): [ 0.01123918 -0.0232506   0.00726745 -0.02499623  0.04277921  0.0561555 ]\n",
      "‚úÖ Raw T5 Encoder States:\n",
      "  Shape: torch.Size([1, 5, 768])\n",
      "  First token (first 10 dims): [-0.22068247  0.36205706 -0.43891594  0.34720415 -0.1919306   0.15727508\n",
      "  0.050558    0.16643807  0.04266588  0.33770296]\n",
      "‚úÖ Gated/Contextualized Encoder States:\n",
      "  First token (first 10 dims): [-0.10420666  0.1696526  -0.21169163  0.17425148 -0.10310978  0.07978344\n",
      "  0.02420234  0.08007693  0.02066433  0.16479082]\n",
      "‚úÖ Personalized Encoder States (after s_hat injection):\n",
      "  First token (first 10 dims): [-0.06386092  0.1890965  -0.14572313  0.18714905 -0.15214641  0.04164901\n",
      " -0.05834827  0.10416329  0.04190599  0.12395056]\n",
      "Gold summary source: S-29150\n",
      "Gold reference summaries: Royal reporter Katie Nicholl tells Entertainment Tonight that Markle \"hasn't been in any hurry to get back in shape,\" and nor should she be!\n",
      "‚úÖ Target Summary Tokens:\n",
      "  Token IDs: [ 3671 17021 20413  2504 14297    40   817     7 12694 24951]\n",
      "‚úÖ Predicted Summary Tokens:\n",
      "  Token IDs: [32099   282    11 22960 14297   739     7     7     3 24951]\n",
      "‚úÖ Losses:\n",
      "  Generation loss (T5): 4.7318\n",
      "  Behavior encoder loss: 158.4887\n",
      "  Total loss: 163.2205\n",
      "\n",
      "‚úÖ Personalized T5 Output:\n",
      "  Target summary: Royal reporter Katie Nicholl tells Entertainment Tonight that Markle \"hasn't been in any hurry to get back in shape,\" and nor should she be!\n",
      "  Predicted summary: As and Holmescholsonss  Tonight: she  isThe 't been  the form to get to to the\"  that does she be.\n",
      "  Total loss: 163.2205\n",
      "  Encoder loss: 158.4887\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ Sample one row\n",
    "# -----------------------------\n",
    "sample_row = train_df.sample(1).iloc[0]\n",
    "Bhist = literal_eval(sample_row['EHist'])   # list of history Doc IDs\n",
    "Bpos = sample_row['EPos']                # the current position doc ID (or use 'Bpos' column if present)\n",
    "print(f\"üîπ Random Row UserID: {sample_row.UserID}\")\n",
    "print(f\"üìù Sample Bhist: {Bhist}\")\n",
    "print(f\"üéØ Target Bpos: {Bpos}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ Initialize models\n",
    "# -----------------------------\n",
    "behavior_encoder = BehaviorEncoder(hidden_dim, tail2idx, device, debug=True).to(device)\n",
    "inverse_decoder = BehaviorInverseDecoderPredict(hidden_dim, device, debug=True).to(device)\n",
    "personalized_model = PersonalizedT5Summarizer(hidden_dim, summarizer_model, behavior_encoder, inverse_decoder, device).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ Forward pass through Behavior Encoder\n",
    "# -----------------------------\n",
    "eprime_last, eprime_next, logits_pos, enc_loss = behavior_encoder(\n",
    "    Bhist, Bpos, lookup_df, tail2idx, embed_tables\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Behavior Encoder Output:\")\n",
    "print(f\"  eprime_last shape: {eprime_last.shape}\")\n",
    "print(f\"  eprime_next shape: {eprime_next.shape}\")\n",
    "if logits_pos is not None:\n",
    "    pred_tail_idx = torch.argmax(logits_pos, dim=-1).item()\n",
    "    pred_tail = idx2tail[pred_tail_idx]\n",
    "    print(f\"  Predicted tail from encoder: {pred_tail}\")\n",
    "else:\n",
    "    print(\"  No predicted tail (Bpos not in lookup)\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4Ô∏è‚É£ Forward pass through Inverse Decoder\n",
    "# -----------------------------\n",
    "head_emb = embed_tables['headline'].get(Bpos, torch.zeros(hidden_dim, device=device))\n",
    "c_prime, s_hat = inverse_decoder(eprime_next, behavior_encoder.b_emb, head_emb)\n",
    "\n",
    "print(\"\\n‚úÖ Inverse Decoder Output:\")\n",
    "print(f\"  c'_pos (first 6 dims): {c_prime.detach().cpu().numpy()[:6]}\")\n",
    "print(f\"  s_hat_pos (first 6 dims): {s_hat.detach().cpu().numpy()[:6]}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5Ô∏è‚É£ Forward pass through Personalized T5\n",
    "# -----------------------------\n",
    "tail_id = lookup_df.loc[Bpos]['Tail']\n",
    "print(\"Target summary id\",tail_id)\n",
    "gold_summary_text = sid2sum.get(tail_id, \"\")\n",
    "\n",
    "total_loss, logits = personalized_model(\n",
    "    Bhist, Bpos, lookup_df, tail2idx, embed_tables, sid2sum, tokenizer, max_len=50\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6Ô∏è‚É£ Decode predicted tokens\n",
    "# -----------------------------\n",
    "pred_summary = tokenizer.decode(torch.argmax(logits, dim=-1)[0], skip_special_tokens=True)\n",
    "print(\"\\n‚úÖ Personalized T5 Output:\")\n",
    "print(f\"  Target summary: {gold_summary_text[:300]}\")\n",
    "print(f\"  Predicted summary: {pred_summary[:300]}\")\n",
    "print(f\"  Total loss: {total_loss.item():.4f}\")\n",
    "print(f\"  Encoder loss: {enc_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2b453884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Royal reporter Katie Nicholl tells Entertainment Tonight that Markle \"hasn\\'t been in any hurry to get back in shape,\" and nor should she be!'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid2sum.get(\"S-29150\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "341ea732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sid2sum.get(\"S-204452\", \"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
